{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Label.csv file\n",
    "labels = pd.read_csv('../data/CICD/Label.csv')\n",
    "# Read the Data.csv file\n",
    "data = pd.read_csv('../data/CICD/Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create X (features) and y (target)\n",
    "X = data\n",
    "y = labels['Label']\n",
    "\n",
    "# Create train/test split with 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print the shapes of the resulting splits\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=20, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data.columns,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = sorted(labels['Label'].unique())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# Binarize the labels for ROC calculation\n",
    "y_test_bin = label_binarize(y_test, classes=class_names)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], rf_classifier.predict_proba(X_test)[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %s' % (roc_auc[i], class_names[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves for Each Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Based on the confusion matrix, the pairs of classes that are most frequently confused with each other are:\")\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix)):\n",
    "        if i != j:\n",
    "            print(f\"* Class {class_names[i]} is confused with Class {class_names[j]} ({conf_matrix[i, j]} instances)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the confusion counts from the confusion matrix\n",
    "confused_pairs = []\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix)):\n",
    "        if i != j and conf_matrix[i, j] > 0:\n",
    "            confused_pairs.append(((class_names[i], class_names[j]), conf_matrix[i, j]))\n",
    "\n",
    "# Sort the confused pairs based on the number of instances in descending order\n",
    "confused_pairs_sorted = sorted(confused_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Pairs of classes that are most frequently confused with each other (in descending order):\")\n",
    "for (pair, count) in confused_pairs_sorted:\n",
    "    print(f\"* Class {pair[0]} is confused with Class {pair[1]} ({count} instances)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 1. Feature importance analysis for class 5\n",
    "\n",
    "# Calculate permutation importance specifically for class 5\n",
    "result = permutation_importance(rf_classifier, X_test, y_test, \n",
    "                              n_repeats=10, \n",
    "                              random_state=42)\n",
    "\n",
    "# Create DataFrame of feature importance specifically for this problem\n",
    "class_importance = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': result.importances_mean\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features for Classification:\")\n",
    "print(class_importance.head(10))\n",
    "\n",
    "# 2. Analyze samples that are misclassified\n",
    "misclassified = X_test[y_test != y_pred]\n",
    "misclassified_true = y_test[y_test != y_pred]\n",
    "misclassified_pred = y_pred[y_test != y_pred]\n",
    "\n",
    "# Focus on class 5 misclassifications\n",
    "class_5_errors = misclassified[\n",
    "    (misclassified_true == 5) | (misclassified_pred == 5)\n",
    "]\n",
    "\n",
    "print(\"\\nStatistical summary of misclassified samples for class 5:\")\n",
    "print(class_5_errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Define custom class weights, increasing weight for class 5\n",
    "class_weights = {\n",
    "    0: 1,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 4,\n",
    "    5: 5,  # Increase weight for class 5\n",
    "    6: 1,\n",
    "    7: 1,\n",
    "    8: 1,\n",
    "    9: 1\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Classifier with custom class weights\n",
    "rf_classifier_weighted = RandomForestClassifier(n_estimators=100,\n",
    "                                                 max_depth=20,\n",
    "                                                 class_weight=class_weights,\n",
    "                                                 random_state=42,\n",
    "                                                 n_jobs=-1)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_classifier_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_weighted = rf_classifier_weighted.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "print(\"Accuracy with Weighted Classes:\", accuracy_weighted)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report with Weighted Classes:\")\n",
    "print(classification_report(y_test, y_pred_weighted))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance_weighted = pd.DataFrame({\n",
    "    'feature': data.columns,\n",
    "    'importance': rf_classifier_weighted.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Weighted):\")\n",
    "print(feature_importance_weighted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "class_names = sorted(labels['Label'].unique())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "import io  # Import the io module\n",
    "import os\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://192.168.1.189:9000\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"roberto\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"patilla1\"\n",
    "\n",
    "\n",
    "# Set remote MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://192.168.1.86:5050\")\n",
    "\n",
    "# Ensure experiment exists or create it\n",
    "mlflow.set_experiment(\"CICD_IDS_Model_v1\")\n",
    "\n",
    "# Start a new run\n",
    "with mlflow.start_run(run_name=\"Stacking_Classifier-rf+lr_and_standard_scaler\") as run:\n",
    "\n",
    "    # Automatically log all parameters, metrics, and models\n",
    "    mlflow.autolog()\n",
    "    \n",
    "    \n",
    "    # Define base estimators\n",
    "    estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=20, class_weight=class_weights, random_state=42, n_jobs=-1)),\n",
    "        ('lr', LogisticRegression(max_iter=1000,random_state=42))\n",
    "    ]\n",
    "\n",
    "    # Create stacking classifier with Logistic Regression as the meta-estimator\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    # Define and train pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', stacking_clf)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate and log metric\n",
    "    y_pred_weighted = pipeline.predict(X_test)\n",
    "    accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_weighted)\n",
    "\n",
    "    # Generate classification report and log it as an artifact\n",
    "    report_text = classification_report(y_test, y_pred_weighted)\n",
    "    \n",
    "    # Use io.StringIO to create an in-memory text buffer\n",
    "    buffer = io.StringIO()\n",
    "    buffer.write(report_text)\n",
    "    \n",
    "    # Log the buffer as an artifact\n",
    "    mlflow.log_text(buffer.getvalue(), \"classification_report.txt\")\n",
    "\n",
    "    # Save and log feature importance\n",
    "    feature_importance_weighted = pd.DataFrame({\n",
    "        'feature': data.columns,\n",
    "        'importance': rf_classifier_weighted.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    feature_importance_weighted.to_json(\"feature_importance.json\", orient=\"records\", indent=2)\n",
    "    mlflow.log_artifact(\"feature_importance.json\")\n",
    "\n",
    "    # Log pipeline and register it\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"CICD_IDS_Model_v1\",\n",
    "        signature=infer_signature(X_train, y_train)\n",
    "    )\n",
    "\n",
    "    print(\"✅ Model, metrics and artifacts logged to remote MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the registered model from MLflow\n",
    "loaded_model = mlflow.sklearn.load_model(\"models:/CICD_IDS_Model_v1/Production\")\n",
    "\n",
    "# Run inference on a subset of test data (e.g., first 10 samples)\n",
    "predictions = loaded_model.predict(X_test.head(10))\n",
    "print(\"Predictions for first 10 test samples:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
